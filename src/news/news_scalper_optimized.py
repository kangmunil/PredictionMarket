"""
News Scalping Trading Engine - OPTIMIZED VERSION
=================================================

Optimized for speed: Target latency < 2 seconds

Optimizations:
1. Parallel news processing (asyncio.gather)
2. Market pre-caching (Redis/memory)
3. Model pre-warming (FinBERT loaded on startup)
4. Batch sentiment analysis
5. Connection pooling

Author: ArbHunter V2 (Optimized)
Created: 2026-01-03
"""

import asyncio
import logging
from typing import Dict, List, Optional
from datetime import datetime, timedelta
from decimal import Decimal
import time
from collections import defaultdict
import os
import json
from dateutil import parser as date_parser

from .news_aggregator import NewsAggregator
from .sentiment_analyzer import SentimentAnalyzer
from .market_matcher import MarketMatcher
from src.core.risk_manager import RiskManager
from src.core.decision_logger import DecisionLogger
from src.core.allocation_manager import AllocationManager

# RAG System (Advanced AI Analysis)
try:
    import sys
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
    from core.rag_system_openrouter import OpenRouterRAGSystem, NewsEvent
    RAG_AVAILABLE = True
except ImportError as e:
    logging.warning(f"‚ö†Ô∏è  RAG System not available: {e}")
    RAG_AVAILABLE = False

logger = logging.getLogger(__name__)


from .news_client import TreeNewsStreamClient

class OptimizedNewsScalper:
    """
    Speed-optimized news scalper.
    """

    def __init__(
        self,
        news_api_key: str,
        tree_news_api_key: str,
        clob_client,
        gamma_api_url: str = "https://gamma-api.polymarket.com",
        budget_manager = None,
        dry_run: bool = True,
        use_rag: bool = True,
        openrouter_api_key: str = None,
        supabase_url: str = None,
        supabase_key: str = None,
        signal_bus = None,
        swarm_system = None
    ):
        # ... existing init code ...
        self.news_aggregator = NewsAggregator(
            news_api_key=news_api_key,
            tree_api_key=tree_news_api_key
        )
        
        # Connect to Hive Mind
        self.signal_bus = signal_bus
        self.swarm_system = swarm_system # üêù Reference to Orchestrator
        if self.signal_bus:
            logger.info("üß† Connected to SignalBus (Hive Mind)")

        self.balance_allocator: Optional[AllocationManager] = None
        if os.getenv("ENABLE_BALANCE_ALLOCATOR", "false").lower() in ("1", "true", "yes", "on"):
            try:
                self.balance_allocator = AllocationManager()
                logger.info("üí∞ Balance-aware allocator enabled")
            except Exception as exc:
                logger.warning(f"‚ö†Ô∏è Could not enable balance allocator: {exc}")
        
        # Real-time Stream Client
        self.stream_client = TreeNewsStreamClient(api_key=tree_news_api_key)
        self.stream_task = None

        # Analysis Engine
        self.use_rag = use_rag and RAG_AVAILABLE
        # ... rest of init ...
        
        # Initialize Risk Manager
        self.risk_manager = RiskManager()
        
        # Initialize Decision Logger
        notifier = self.swarm_system.notifier if self.swarm_system else None
        self.decision_logger = DecisionLogger("NewsScalper", notifier=notifier)
        
        logger.info("üõ°Ô∏è Risk Manager Initialized (Dynamic Kelly Sizing)")

        if self.use_rag:
            logger.info("ü§ñ Initializing RAG System (Advanced AI Analysis)...")
            self.rag_system = OpenRouterRAGSystem(
                openrouter_api_key=openrouter_api_key or os.getenv("OPENROUTER_API_KEY"),
                supabase_url=supabase_url or os.getenv("SUPABASE_URL"),
                supabase_key=supabase_key or os.getenv("SUPABASE_KEY")
            )
            self.sentiment_analyzer = None  # Not used in RAG mode
            logger.info("   ‚úÖ RAG System initialized")
        else:
            logger.info("üß† Using Basic FinBERT Sentiment Analysis...")
            self.sentiment_analyzer = SentimentAnalyzer()
            self.rag_system = None
            logger.info("   ‚úÖ FinBERT initialized")

        self.market_matcher = MarketMatcher(gamma_api_url=gamma_api_url)
        self.clob_client = clob_client
        self.budget_manager = budget_manager
        self.dry_run = dry_run

        # Bridge: Connects News to Stat Arb
        try:
            from src.strategies.stat_arb_enhanced import EnhancedStatArbStrategy
            from src.strategies.news_arb_bridge import NewsToArbBridge
            stat_strategy = EnhancedStatArbStrategy(client=clob_client)
            self.bridge = NewsToArbBridge(self, stat_strategy, self.market_matcher)
            logger.info("üåâ News-to-Arb Bridge initialized")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Bridge not available: {e}")
            self.bridge = None

        # Trading config (OPTIMIZED FOR 50% PROBABILITY)
        self.min_confidence = 0.50       # 50% ÌôïÎ•†Ïù¥Î©¥ ÏßÑÏûÖ
        self.high_impact_threshold = 0.70 # 70% Ïù¥ÏÉÅÏù¥Î©¥ Í≥†ÏòÅÌñ• Îâ¥Ïä§Î°ú ÌåêÎã®
        self.min_market_volume = 1.0     # Í±∞ÎûòÎüâ Î¨∏ÌÑ±ÏùÑ ÎÇÆÏ∂∞ Îçî ÎßéÏùÄ ÏãúÏû• Îß§Ïπ≠
        self.position_size = 10.0        # Í∏∞Î≥∏ ÏßÑÏûÖ Í∏àÏï° $10
        self.max_positions = 5
        self.stop_loss_pct = -0.05  # -5% stop-loss (Í∂åÏû•: -3% ~ -5%)

        # Performance optimization: Market cache
        self.market_cache = {}  # keyword -> [markets]
        self.cache_ttl = 300  # 5 minutes
        self.cache_timestamps = {}

        # Performance optimization: Pre-loaded markets
        self.preloaded_markets = defaultdict(list)

        # State tracking
        self.positions = {}
        self.processed_news = set()
        self.stats = {
            "news_checked": 0,
            "signals_generated": 0,
            "trades_executed": 0,
            "positions_opened": 0,
            "positions_closed": 0,
            "total_pnl": 0.0,
            "wins": 0,
            "losses": 0,
            "avg_latency_ms": 0.0,
            "latencies": []
        }

        self.start_time = None
        self._is_warmed_up = False

    async def warmup(self, keywords: List[str]):
        """
        Pre-warm models and cache markets.

        This reduces first-request latency from 3-5s to <500ms.
        """
        logger.info("üî• Warming up system...")

        # 1. Pre-load AI Model (RAG or FinBERT)
        if self.use_rag:
            logger.info("   Initializing RAG System (OpenRouter AI + ChromaDB + Supabase)...")
            start = time.time()
            # RAG System initialization (connects to Supabase, ChromaDB)
            # This is done in __init__, just log status
            logger.info(f"   ‚úÖ RAG System ready ({time.time() - start:.1f}s)")
            logger.info(f"      - Entity Model: {os.getenv('AI_MODEL_ENTITY', 'claude-3-haiku')}")
            logger.info(f"      - Analysis Model: {os.getenv('AI_MODEL_ANALYSIS', 'claude-3.5-sonnet')}")
            logger.info(f"      - Vector Store: ChromaDB")
            logger.info(f"      - Database: Supabase")
        else:
            logger.info("   Loading FinBERT model...")
            start = time.time()
            self.sentiment_analyzer.analyze("Test headline for warming up")
            logger.info(f"   ‚úÖ FinBERT loaded ({time.time() - start:.1f}s)")

        # 2. Pre-cache markets for keywords
        logger.info("   Pre-caching markets...")
        start = time.time()

        # Cache common markets
        for keyword in keywords:
            markets = await self.market_matcher.find_matching_markets(
                keyword,
                min_volume=10.0,  # Low threshold for caching
                max_results=20
            )
            if markets:
                self.preloaded_markets[keyword] = markets
                logger.info(f"      Cached {len(markets)} markets for '{keyword}'")

        logger.info(f"   ‚úÖ Markets cached ({time.time() - start:.1f}s)")

        self._is_warmed_up = True
        mode = "RAG (Advanced AI)" if self.use_rag else "FinBERT (Basic)"
        logger.info(f"‚úÖ Warmup complete! Mode: {mode}")

    async def validate_trade(self, market_id: str, outcome: str, price: float) -> bool:
        """
        AI Validator Interface for WalletWatcher.
        Checks if recent news supports this trade.
        """
        # 1. Fetch recent news for this market/topic
        # (Simplified: assumes market_id is related to monitored keywords or extracts entity)
        # For now, we do a quick sentiment check on Cached news or latest headlines
        
        # TODO: Extract entity from market_id properly
        entity = "bitcoin" # Placeholder: In real logic, query CLOB for market question
        
        # Quick sentiment check using cache or latest analysis
        # If we have positive sentiment in cache for this entity, APPROVED.
        
        # Check if we have active signals/positions for this entity in recent memory
        # Or re-run sentiment on latest news
        
        # For prototype: Return True if we have ANY positive signals recently
        if self.stats["signals_generated"] > 0:
             # Just an example logic: If scalper is finding signals, market is hot.
             # Real logic: specific entity matching.
             logger.info(f"   üß† AI Brain: Market is active (Signals: {self.stats['signals_generated']}). Validation PASS.")
             return True
             
        # Fallback: Neutral
        return True # Default PASS for now to allow copy trading in demo

    async def run(
        self,
        keywords: List[str],
        check_interval: int = 60,
        max_runtime: Optional[int] = None
    ):
        """Main monitoring loop with optimizations"""

        logger.info("=" * 80)
        logger.info("üöÄ OPTIMIZED NEWS SCALPING BOT STARTED")
        logger.info("=" * 80)
        logger.info(f"Mode: {'DRY RUN' if self.dry_run else 'LIVE TRADING'}")
        logger.info(f"Keywords: {keywords}")
        logger.info(f"Check Interval: {check_interval}s")
        logger.info("üì° Real-time WebSocket: ENABLED")
        logger.info("=" * 80)

        # Warmup first
        if not self._is_warmed_up:
            await self.warmup(keywords)

        self.start_time = datetime.now()
        end_time = None
        if max_runtime:
            end_time = self.start_time + timedelta(seconds=max_runtime)

        # START WebSocket Stream Task in Background
        self.stream_task = asyncio.create_task(self._run_news_stream(keywords))

        try:
            while True:
                if end_time and datetime.now() >= end_time:
                    logger.info(f"‚è∞ Reached max runtime ({max_runtime}s). Stopping.")
                    break

                # Check news (polling fallback)
                await self._check_news_parallel(keywords)

                # Monitor positions
                await self._monitor_positions()

                # Print stats
                self._print_stats()

                # Wait
                await asyncio.sleep(check_interval)

        except KeyboardInterrupt:
            logger.info("\n‚ö†Ô∏è  Keyboard interrupt received. Shutting down...")
        except Exception as e:
            logger.error(f"‚ùå Fatal error: {e}")
            import traceback
            logger.error(traceback.format_exc())
        finally:
            # Stop stream task
            if self.stream_task:
                self.stream_task.cancel()
            await self._shutdown()

    async def _run_news_stream(self, keywords: List[str]):
        """
        Background task to process real-time news stream.
        """
        logger.info("üßµ Starting real-time news stream task...")
        try:
            async for article in self.stream_client.stream_news():
                # Filter by keywords (Tree News stream sends EVERYTHING)
                title = article.get("title", "").lower()
                content = article.get("content", "").lower()
                
                if any(kw.lower() in title or kw.lower() in content for kw in keywords):
                    logger.info(f"üî• REAL-TIME NEWS DETECTED: {article.get('title')[:60]}...")
                    # Process immediately!
                    asyncio.create_task(self._process_article_optimized(article))
                else:
                    # Log ignored news at debug level
                    logger.debug(f"üîá Ignored non-target news: {article.get('title')[:40]}...")
                    
        except asyncio.CancelledError:
            logger.info("üßµ Real-time news stream task cancelled")
        except Exception as e:
            logger.error(f"‚ùå Real-time news stream error: {e}")

    async def _check_news_parallel(self, keywords: List[str]):
        """
        OPTIMIZED: Process news articles in parallel from multiple sources.

        Speedup: 5-10x faster than sequential processing.
        Multi-source: NewsAPI + Tree News for redundancy and speed.
        """
        try:
            # Fetch news from all sources (NewsAPI + Tree News)
            articles = await self.news_aggregator.get_breaking_news(
                keywords=keywords,
                max_results=20
            )

            self.stats["news_checked"] += len(articles)

            if not articles:
                return

            logger.info(f"üì∞ Processing {len(articles)} articles in parallel...")

            # OPTIMIZATION: Process all articles concurrently
            tasks = [
                self._process_article_optimized(article)
                for article in articles
            ]

            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Count successes
            successful = sum(1 for r in results if r and not isinstance(r, Exception))
            logger.info(f"   ‚úÖ Processed {successful}/{len(articles)} articles")

        except Exception as e:
            logger.error(f"‚ùå Error checking news: {e}")

    async def _process_article_optimized(self, article: Dict) -> Optional[Dict]:
        """
        OPTIMIZED: Process article with timing and caching.

        Uses RAG System (Advanced AI) or FinBERT (Basic) depending on configuration.

        Target: < 2 seconds total
        """
        url = article.get("url", "")
        title = article.get("title", "")
        content = article.get("description", "") or article.get("content", "")

        # Skip if processed
        if url in self.processed_news:
            return None

        self.processed_news.add(url)

        # Start latency timer
        start_time = time.time()

        try:
            # === RAG MODE: Advanced AI Analysis ===
            if self.use_rag:
                # 2. Convert article to NewsEvent
                # Safely handle source (TreeNews uses string, NewsAPI uses dict)
                source_data = article.get("source", "Unknown")
                source_name = source_data.get("name", "Unknown") if isinstance(source_data, dict) else str(source_data)

                # Use dateutil.parser for flexible date parsing
                try:
                    pub_date = date_parser.parse(article.get("publishedAt", ""))
                except:
                    pub_date = datetime.now()

                event = NewsEvent(
                    event_id=f"news_{hash(url)}",
                    title=title,
                    content=content,
                    source=source_name,
                    published_at=pub_date,
                    entities=[],  # Will be extracted by RAG
                    category="crypto",  # Default
                    url=url
                )

                # 2. Extract entities using LLM (Dynamic extraction)
                llm_entities = await self.rag_system.extract_entities(event)
                event.entities = llm_entities # Update event with extracted entities
                
                if not llm_entities:
                    logger.debug(f"   ‚ö†Ô∏è  No entities extracted by AI for: {title[:30]}...")
                    # Optional: Fallback to regex is handled inside _find_markets_cached if override_keywords is None/Empty
                    # But here we explicitly pass what we got.
                
                # 3. Find relevant markets using AI-extracted keywords
                markets = await self._find_markets_cached(title, override_keywords=llm_entities)
                
                if not markets:
                    logger.debug(f"   ‚ö†Ô∏è  No matching markets found for entities: {llm_entities}")
                    return None

                # 4. Analyze market impact with RAG System
                best_market = markets[0]
                market_id = best_market.get("condition_id", "")
                market_question = best_market.get("question", "")

                # Get current price from market
                try:
                    outcome_prices = json.loads(best_market.get("outcomePrices", "[]")) if isinstance(best_market.get("outcomePrices"), str) else best_market.get("outcomePrices", [])
                    if outcome_prices and len(outcome_prices) > 0:
                        current_price = Decimal(str(outcome_prices[0]))
                    else:
                        current_price = Decimal("0.5")
                except:
                    current_price = Decimal("0.5")

                impact = await self.rag_system.analyze_market_impact(
                    event=event,
                    market_id=market_id,
                    current_price=current_price,
                    market_question=market_question
                )

                # 5. Check if tradeable
                if impact.confidence < self.min_confidence:
                    logger.debug(f"   ‚ö†Ô∏è  Low confidence: {impact.confidence:.1%}")
                    return None

                if impact.trade_recommendation == "hold":
                    logger.debug(f"   ‚ö†Ô∏è  Recommendation: HOLD")
                    return None

                # 6. Convert RAG output to trade signal
                label = impact.trade_recommendation  # "buy" or "sell"
                score = impact.confidence
                is_high_impact = score >= self.high_impact_threshold

                # Record latency
                latency_ms = (time.time() - start_time) * 1000
                self.stats["latencies"].append(latency_ms)

                logger.info(f"   ‚úÖ RAG SIGNAL! {label.upper()} ({score:.1%}) - {latency_ms:.0f}ms")
                logger.info(f"      Market: {best_market.get('question', '')[:60]}...")
                logger.info(f"      Reasoning: {impact.reasoning[:100]}...")

                self.stats["signals_generated"] += 1
                
                # üß† HIVE MIND UPDATE: Broadcast signal to other bots
                if self.signal_bus:
                    token_id = None
                    # Try to get token ID for signal bus
                    try:
                        clob_ids = json.loads(best_market.get("clobTokenIds", "[]")) if isinstance(best_market.get("clobTokenIds"), str) else best_market.get("clobTokenIds", [])
                        if clob_ids: token_id = clob_ids[0]
                    except: pass
                    
                    if token_id:
                        asyncio.create_task(self.signal_bus.update_signal(
                            token_id=token_id,
                            source='NEWS',
                            score=score if label == 'buy' else -score,
                            label=label
                        ))

                # 7. Execute trade with RAG-enhanced signal
                sentiment = {
                    "label": label,
                    "score": score,
                    "reasoning": impact.reasoning,
                    "model": impact.model_used
                }

                await self._execute_trade(
                    article=article,
                    sentiment=sentiment,
                    market=best_market,
                    is_high_impact=is_high_impact
                )

                return {"success": True, "latency_ms": latency_ms, "mode": "RAG"}

            # === BASIC MODE: FinBERT Sentiment Analysis ===
            else:
                # 1. Sentiment analysis (cached model)
                sentiment = self.sentiment_analyzer.analyze(title)
                label = sentiment["label"]
                score = sentiment["score"]

                # BRIDGE TRIGGER: Run Stat Arb analysis based on news sentiment
                if self.bridge and score >= self.min_confidence and label != "neutral":
                    # We run this in background to avoid blocking direct trade execution
                    asyncio.create_task(self.bridge.on_high_impact_news(article, sentiment))

                # Detailed debug logging for decision process
                if score < self.min_confidence:
                    logger.debug(f"   ‚è≠Ô∏è  Skipped '{title[:30]}...': Low confidence ({score:.1%} < {self.min_confidence:.0%})")
                    return None
                
                if label == "neutral":
                    logger.debug(f"   ‚è≠Ô∏è  Skipped '{title[:30]}...': Neutral sentiment")
                    return None

                # 2. Find markets (use cache if available)
                markets = await self._find_markets_cached(title)

                if not markets:
                    logger.info(f"   üîç No matching markets for: '{title[:40]}...' (Score: {score:.1%})")
                    return None

                # 3. Execute trade
                is_high_impact = score >= self.high_impact_threshold
                best_market = markets[0]

                # Record latency
                latency_ms = (time.time() - start_time) * 1000
                self.stats["latencies"].append(latency_ms)

                logger.info(f"   ‚úÖ SIGNAL! {label.upper()} ({score:.1%}) - {latency_ms:.0f}ms")
                logger.info(f"      Market: {best_market.get('question', '')[:60]}...")

                self.stats["signals_generated"] += 1

                # Execute trade
                await self._execute_trade(
                    article=article,
                    sentiment=sentiment,
                    market=best_market,
                    is_high_impact=is_high_impact
                )

                return {"success": True, "latency_ms": latency_ms, "mode": "FinBERT"}

        except Exception as e:
            logger.error(f"‚ùå Error processing article: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return None

    async def _find_markets_cached(self, news_text: str, override_keywords: Optional[List[str]] = None) -> List[Dict]:
        """
        OPTIMIZED: Find markets using cache.

        Speedup: 10-100x (no API call if cached)
        """
        # Use provided keywords or extract using regex
        if override_keywords:
             search_keywords = override_keywords
             # Also try to extract categories for cache lookup (heuristic)
             entities = {"manual": override_keywords} 
        else:
            entities = self.market_matcher.extract_entities(news_text)
            search_keywords = []
            for kw_list in entities.values():
                search_keywords.extend(kw_list)

        # Check pre-loaded cache first
        for kw in search_keywords:
             if kw in self.preloaded_markets:
                 logger.debug(f"   üí® Cache hit for '{kw}'")
                 return self.preloaded_markets[kw]

        # Fallback to API search
        markets = await self.market_matcher.find_matching_markets(
            news_text,
            min_volume=self.min_market_volume,
            max_results=3,
            override_keywords=override_keywords
        )

        # Cache result
        if markets and search_keywords:
            for kw in search_keywords:
                self.preloaded_markets[kw] = markets
                # Cache only under first keyword to avoid duplication overhead, or all?
                # All is safer for hits.
                
        return markets

    async def _execute_trade(
        self,
        article: Dict,
        sentiment: Dict,
        market: Dict,
        is_high_impact: bool
    ):
        """Execute trade with improved token ID extraction"""
        market_question = market.get('question', '')
        logger.info(f"üöÄ [SCALPER] Entering _execute_trade for '{market_question[:30]}...'")
        
        allocation_id: Optional[str] = None
        allocation_amount: Optional[Decimal] = None
        position_opened = False

        try:
            # üöÄ Ïú†Ïó∞Ìïú Î†àÏù¥Î∏î Ï≤òÎ¶¨ (AI Ï∂úÎ†•Í∞í ÎåÄÏùë)
            raw_label = str(sentiment.get("label", "")).lower()
            logger.info(f"   üîç Analyzing Signal Label: '{raw_label}'")
            
            if raw_label in ["positive", "buy"]:
                side = "BUY"
            elif raw_label in ["negative", "sell"]:
                side = "SELL"
            else:
                logger.info(f"   ‚è≠Ô∏è  Skipping: Label '{raw_label}' is not actionable (HOLD)")
                return

            # Robust Token ID extraction (matching market_matcher.py logic)
            token_id = None
            clob_ids_raw = market.get("clobTokenIds", [])
            
            if isinstance(clob_ids_raw, str) and clob_ids_raw:
                try:
                    import json
                    clob_ids = json.loads(clob_ids_raw)
                    if clob_ids: token_id = clob_ids[0]
                except: pass
            elif isinstance(clob_ids_raw, list) and clob_ids_raw:
                token_id = clob_ids_raw[0]

            # Fallback: Condition IDÎ•º Token IDÎ°ú Î≥ÄÌôò ÏãúÎèÑ
            if not token_id:
                cid = market.get("condition_id") or market.get("id")
                if cid:
                    logger.debug(f"   üîç Token ID missing, attempting to resolve from CID: {cid[:10]}")
                    token_id = self.clob_client.get_yes_token_id(cid)

            if not token_id:
                logger.warning(f"   ‚ö†Ô∏è  [EXECUTION ABORTED] Could not extract valid Token ID for: {market_question[:30]}...")
                return

            logger.info(f"   ‚úÖ Target Token Resolved: {token_id[:15]}...")

            # üõ°Ô∏è DYNAMIC POSITION SIZING (Kelly Criterion)
            current_price = await self._get_current_price(token_id)
            if current_price <= 0: current_price = 0.5 # Safety fallback
            
            position_size = self.risk_manager.calculate_position_size(
                prob_win=sentiment["score"],
                current_price=current_price,
                category="crypto", # Can be dynamic based on news
                volatility_score=0.1 # Placeholder: would calculate from spread
            )
            
            if position_size <= 0:
                logger.info(f"   üõë [RISK REJECTED] Size: ${position_size:.2f} | Reason: Low EV (Prob {sentiment['score']:.2f} vs Price {current_price:.2f}) or Volatility Penalty")
                return

            if self.balance_allocator:
                adjusted_size = await self.balance_allocator.allocate_for_market(
                    token_id,
                    position_size
                )
                if adjusted_size <= 0:
                    logger.info("   ‚ö†Ô∏è Allocation manager rejected trade for insufficient budget")
                    return
                position_size = adjusted_size

            # üí∞ BUDGET ALLOCATION
            if self.budget_manager:
                allocation_amount = Decimal(str(position_size))
                allocation_id = await self.budget_manager.request_allocation(
                    strategy="arbhunter",
                    amount=allocation_amount,
                    priority="high" if is_high_impact else "normal"
                )
                if not allocation_id:
                    logger.warning(f"   ‚ö†Ô∏è  Budget allocation denied. Skipping trade.")
                    return
                logger.debug(f"   üí∞ Allocation approved: {allocation_id}")

            # üß† Log Decision using centralized logger
            await self.decision_logger.log_decision(
                action=side,
                token=market_question[:40],
                confidence=sentiment["score"],
                reason=sentiment.get("reasoning", "High impact news detected"),
                factors={
                    "News": article.get("title", "")[:50],
                    "Price": f"${current_price:.3f}",
                    "Size": f"${position_size:.2f}",
                    "Model": sentiment.get("model", "FinBERT"),
                    "Token ID": token_id
                }
            )

            if self.dry_run:
                # PAPER TRADING: Get real entry price
                entry_price = await self._get_current_price(token_id)

                logger.info(f"   üß™ PAPER TRADING: Would execute slippage-protected {side} order")
                logger.info(f"      Entry price: ${entry_price:.4f}")
                logger.info(f"      Max slippage: 2.0%")
                logger.info(f"      Order type: IOC Limit")

                # Track position
                self.positions[token_id] = {
                    "token_id": token_id,
                    "market": market,
                    "side": side,
                    "size": position_size,
                    "entry_price": entry_price,  # Real market price
                    "entry_time": datetime.now(),
                    "sentiment": sentiment,
                    "article": article,
                    "is_high_impact": is_high_impact,
                    "allocation_id": allocation_id,
                    "allocation_amount": allocation_amount
                }
                self.stats["trades_executed"] += 1
                self.stats["positions_opened"] += 1
                position_opened = True

                # üêù HIVE MIND HISTORY: Report to orchestrator
                if self.swarm_system:
                    self.swarm_system.add_trade_record(side, token_id, entry_price, position_size)
                    # üöÄ PnL Tracker Í∏∞Î°ù Ï∂îÍ∞Ä
                    if hasattr(self.swarm_system, 'pnl_tracker'):
                        self.positions[token_id]["pnl_tid"] = self.swarm_system.pnl_tracker.record_entry(
                            strategy="news_scalper",
                            token_id=token_id,
                            side=side,
                            price=entry_price,
                            size=position_size
                        )
            else:
                # LIVE MODE: Use slippage-protected order
                logger.info(f"   üõ°Ô∏è  Using slippage protection (max 2%)")

                order_result = await self.clob_client.place_limit_order_with_slippage_protection(
                    token_id=token_id,
                    side=side,
                    amount=position_size,
                    max_slippage_pct=2.0,  # Max 2% slippage
                    priority="high" if is_high_impact else "normal"
                )

                if order_result:
                    logger.info(f"   ‚úÖ Order executed with slippage protection")
                    entry_price = float(order_result.get('price', 0.5))

                    # Track position
                    self.positions[token_id] = {
                        "token_id": token_id,
                        "market": market,
                        "side": side,
                        "size": position_size,
                        "entry_price": entry_price,
                        "entry_time": datetime.now(),
                        "sentiment": sentiment,
                        "article": article,
                        "is_high_impact": is_high_impact,
                        "order_id": order_result.get('orderID'),
                        "allocation_id": allocation_id,
                        "allocation_amount": allocation_amount
                    }
                    self.stats["trades_executed"] += 1
                    self.stats["positions_opened"] += 1
                    position_opened = True
                    
                    # üöÄ PnL Tracker Í∏∞Î°ù Ï∂îÍ∞Ä (Live)
                    if self.swarm_system and hasattr(self.swarm_system, 'pnl_tracker'):
                        self.positions[token_id]["pnl_tid"] = self.swarm_system.pnl_tracker.record_entry(
                            strategy="news_scalper",
                            token_id=token_id,
                            side=side,
                            price=entry_price,
                            size=position_size
                        )
                else:
                    logger.warning(f"   ‚ö†Ô∏è  Order cancelled (slippage too high)")

        except Exception as e:
            logger.error(f"‚ùå Trade execution error: {e}")
            import traceback
            logger.debug(traceback.format_exc())
        finally:
            if self.budget_manager and allocation_id and not position_opened:
                await self.budget_manager.release_allocation("arbhunter", allocation_id, Decimal("0"))

    async def _monitor_positions(self):
        """Monitor positions (same as original)"""
        if not self.positions:
            return

        for token_id, position in list(self.positions.items()):
            try:
                await self._check_position_exit(token_id, position)
            except Exception as e:
                logger.error(f"‚ùå Error monitoring position: {e}")

    async def _check_position_exit(self, token_id: str, position: Dict):
        """
        Check position exit conditions.

        Checks in priority order:
        1. Stop-Loss (P&L based) - Í∏âÍ≤©Ìïú ÏÜêÏã§ Î∞©ÏßÄ
        2. Time-based exit - Îâ¥Ïä§ Ïû¨Î£å ÏÜåÎ©∏ ÎåÄÏùë
        """
        entry_time = position["entry_time"]
        entry_price = position["entry_price"]
        side = position["side"]

        # 1. Stop-Loss Check (P&L based)
        current_price = await self._get_current_price(token_id)

        # Calculate P&L percentage
        if side == "BUY":
            # Long position: profit if price goes up
            pnl_pct = (current_price - entry_price) / entry_price
        else:
            # Short position: profit if price goes down
            pnl_pct = (entry_price - current_price) / entry_price

        # Trigger stop-loss if P&L drops below threshold
        if pnl_pct <= self.stop_loss_pct:
            logger.warning(f"üõë Stop-Loss triggered for {token_id[:16]}...")
            logger.warning(f"   Entry: ${entry_price:.4f}")
            logger.warning(f"   Current: ${current_price:.4f}")
            logger.warning(f"   P&L: {pnl_pct:.2%} (threshold: {self.stop_loss_pct:.2%})")
            await self._close_position(token_id, position, f"Stop-loss ({pnl_pct:.2%})")
            return

        # 2. Time-based exit check
        hold_duration = (datetime.now() - entry_time).total_seconds() / 3600
        # Force exit after 90 minutes regardless of signal class
        max_hold = 1.5

        if hold_duration >= max_hold:
            await self._close_position(token_id, position, f"Max hold time ({max_hold}h)")

    async def _get_current_price(self, token_id: str) -> float:
        """
        Get current market price for token.

        Uses CLOB orderbook to get best bid/ask price.
        """
        try:
            if self.clob_client:
                # Live mode: Use CLOB orderbook
                book = self.clob_client.rest_client.get_order_book(token_id)
                if book.bids and len(book.bids) > 0:
                    return float(book.bids[0].price)
                elif book.asks and len(book.asks) > 0:
                    return float(book.asks[0].price)

            # Fallback: Use mid-price from market data
            import requests
            url = f"https://clob.polymarket.com/price"
            params = {"token_id": token_id}
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: requests.get(url, params=params, timeout=5)
            )

            if response.status_code == 200:
                data = response.json()
                return float(data.get("price", 0.5))

            # Last resort: assume 50%
            return 0.5

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Failed to get current price for {token_id}: {e}")
            return 0.5

    async def _close_position(self, token_id: str, position: Dict, reason: str):
        """
        Close position with REAL P&L calculation (Paper Trading).

        Queries actual market price to calculate realistic P&L.
        """
        try:
            logger.info(f"\nüö™ Closing position: {reason}")

            if self.dry_run:
                # PAPER TRADING: Get real market price
                current_price = await self._get_current_price(token_id)
                entry_price = position["entry_price"]
                size = position["size"]
                side = position["side"]

                # Calculate REAL P&L based on actual price movement
                if side == "BUY":
                    # Long position: profit if price went up
                    price_change = current_price - entry_price
                    pnl = (price_change / entry_price) * size
                else:
                    # Short position: profit if price went down
                    price_change = entry_price - current_price
                    pnl = (price_change / entry_price) * size

                # Track win/loss
                if pnl > 0:
                    self.stats["wins"] += 1
                    result = "WIN"
                else:
                    self.stats["losses"] += 1
                    result = "LOSS"

                self.stats["total_pnl"] += pnl

                # Calculate win rate
                total_trades = self.stats["wins"] + self.stats["losses"]
                win_rate = (self.stats["wins"] / total_trades * 100) if total_trades > 0 else 0

                # Log detailed results
                logger.info(f"   üìä Paper Trading Results:")
                logger.info(f"      Entry: ${entry_price:.4f}")
                logger.info(f"      Exit:  ${current_price:.4f}")
                logger.info(f"      Move:  {((current_price - entry_price) / entry_price * 100):+.2f}%")
                logger.info(f"      P&L:   ${pnl:+.2f} ({result})")
                logger.info(f"      Win Rate: {win_rate:.1f}% ({self.stats['wins']}/{total_trades})")
            else:
                # LIVE MODE: Close actual position
                if self.clob_client:
                    # Execute close order
                    opposite_side = "SELL" if position["side"] == "BUY" else "BUY"
                    await self.clob_client.place_market_order(
                        token_id=token_id,
                        side=opposite_side,
                        amount=position["size"]
                    )
                    logger.info(f"   ‚úÖ Position closed")

            del self.positions[token_id]
            self.stats["positions_closed"] += 1
            
            allocation_id = position.get("allocation_id")
            if self.budget_manager and allocation_id:
                await self.budget_manager.release_allocation(
                    "arbhunter",
                    allocation_id,
                    Decimal("0")
                )

            # üöÄ PnL Tracker Ï≤≠ÏÇ∞ Í∏∞Î°ù Ï∂îÍ∞Ä
            if self.swarm_system and hasattr(self.swarm_system, 'pnl_tracker') and "pnl_tid" in position:
                self.swarm_system.pnl_tracker.record_exit(
                    trade_id=position["pnl_tid"],
                    exit_price=current_price,
                    reason=reason
                )

        except Exception as e:
            logger.error(f"‚ùå Error closing position: {e}")

    def _print_stats(self):
        """Print stats with latency metrics"""
        runtime = (datetime.now() - self.start_time).total_seconds() / 60

        # Calculate average latency
        if self.stats["latencies"]:
            avg_latency = sum(self.stats["latencies"]) / len(self.stats["latencies"])
            self.stats["avg_latency_ms"] = avg_latency
        else:
            avg_latency = 0.0

        logger.debug(f"\nüìà Stats (Runtime: {runtime:.1f}m):")
        logger.debug(f"   News Checked: {self.stats['news_checked']}")
        logger.debug(f"   Signals: {self.stats['signals_generated']}")
        logger.debug(f"   Trades: {self.stats['trades_executed']}")
        logger.debug(f"   Open Positions: {len(self.positions)}")
        logger.debug(f"   Avg Latency: {avg_latency:.0f}ms")

    async def _shutdown(self):
        """Shutdown with performance report"""
        logger.info("\n" + "=" * 80)
        logger.info("üõë SHUTTING DOWN")
        logger.info("=" * 80)

        # Close positions
        if self.positions:
            for token_id, position in list(self.positions.items()):
                await self._close_position(token_id, position, "Shutdown")

        # Final stats
        runtime = (datetime.now() - self.start_time).total_seconds() / 60
        avg_latency = self.stats["avg_latency_ms"]

        # Calculate win rate
        total_trades = self.stats['wins'] + self.stats['losses']
        win_rate = (self.stats['wins'] / total_trades * 100) if total_trades > 0 else 0

        logger.info(f"\nüìä Final Performance Report:")
        logger.info(f"   Runtime: {runtime:.1f}m")
        logger.info(f"   News Checked: {self.stats['news_checked']}")
        logger.info(f"   Signals Generated: {self.stats['signals_generated']}")
        logger.info(f"   Trades Executed: {self.stats['trades_executed']}")
        logger.info(f"   Positions Closed: {self.stats['positions_closed']}")

        logger.info(f"\nüí∞ Trading Results:")
        logger.info(f"   Total P&L: ${self.stats['total_pnl']:+.2f}")
        logger.info(f"   Wins: {self.stats['wins']}")
        logger.info(f"   Losses: {self.stats['losses']}")
        logger.info(f"   Win Rate: {win_rate:.1f}%")
        if total_trades > 0:
            avg_pnl_per_trade = self.stats['total_pnl'] / total_trades
            logger.info(f"   Avg P&L per trade: ${avg_pnl_per_trade:+.2f}")

        logger.info(f"\n‚ö° Performance:")
        logger.info(f"   Average Latency: {avg_latency:.0f}ms")
        logger.info(f"   Target: <2000ms")
        logger.info(f"   Status: {'‚úÖ PASS' if avg_latency < 2000 else '‚ö†Ô∏è  SLOW'}")
        logger.info("=" * 80)
